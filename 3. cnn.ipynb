{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fa5bec-f6db-41df-9804-fbc06708f411",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e524210c-3a06-4f0e-a622-daec269c0030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d138f-9382-4e01-9b7f-07d40c44c69d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading and preparing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b772a1-e2a5-4e47-986e-223a089debce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_2024.csv\", quoting = 3)\n",
    "\n",
    "@keras.saving.register_keras_serializable(name = \"preprocessing\")\n",
    "def preprocessing(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    plain_text = tf.strings.regex_replace(lowercase, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "    return plain_text\n",
    "\n",
    "train_comments = tf.convert_to_tensor(train_data['text'])    # type tf.Tensor\n",
    "train_labels = tf.convert_to_tensor(train_data['label'])     # type tf.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99071c3b-e848-429f-af34-e2e6755bfc13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37da4fbc-f29e-4d0c-971b-218843059b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "epochs = 5\n",
    "\n",
    "# Vectorization layer\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize = preprocessing,\n",
    "    max_tokens = max_features,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9de2aa-ba57-4bf0-ae5b-cafddc49681a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text (InputLayer)           [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 500)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 500, 128)          2560128   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 500, 128)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 165, 128)          114816    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 53, 128)           114816    \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2806401 (10.71 MB)\n",
      "Trainable params: 2806401 (10.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The current architecture is as follows:\n",
    "# (1) Vectorization\n",
    "# (2) Embedding\n",
    "# (3) Dropout\n",
    "# (4) Conv1d\n",
    "# (5) Conv1d\n",
    "# (6) MaxPool1d\n",
    "\n",
    "text_input = keras.Input(shape = (1,), dtype = tf.string, name = 'text')\n",
    "x = vectorize_layer(text_input)\n",
    "\n",
    "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding = \"valid\", activation = \"relu\", strides = 3)(x)\n",
    "x = layers.Conv1D(128, 7, padding = \"valid\", activation = \"relu\", strides = 3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation = \"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation = \"sigmoid\", name = \"predictions\")(x)\n",
    "\n",
    "model = keras.Model(text_input, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809a756-37f9-4369-83cb-ee04c940dd97",
   "metadata": {},
   "source": [
    "#### Fitting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57a0d5-a1b0-426e-93c5-60b5db7b430d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3094/3094 [==============================] - 659s 213ms/step - loss: 0.0680 - accuracy: 0.9757\n",
      "Epoch 2/5\n",
      "3094/3094 [==============================] - 646s 209ms/step - loss: 0.0554 - accuracy: 0.9810\n",
      "Epoch 3/5\n",
      "1943/3094 [=================>............] - ETA: 4:00 - loss: 0.0460 - accuracy: 0.9843"
     ]
    }
   ],
   "source": [
    "model.fit(train_comments, train_labels, epochs = epochs)\n",
    "model.save('keras_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a67b1-9636-40e4-bd0b-b18eb587f767",
   "metadata": {},
   "source": [
    "#### Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35005d7a-0721-4f7e-865c-84f59a03d719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv(\"dev_2024.csv\", quoting = 3)\n",
    "valid_comments = tf.convert_to_tensor(valid_data['text'])     # type tf.Tensor\n",
    "valid_labels = tf.convert_to_tensor(valid_data['label'])      # type tf.Tensor\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(valid_comments, valid_labels, verbose = 0)\n",
    "print(f\"Loss of the model: {eval_loss}\")\n",
    "print(f\"Accuracy of the model: {eval_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd1d4c-8e48-4b68-9cb2-98d9cf95b56c",
   "metadata": {},
   "source": [
    "#### Feeding the model with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbc00f-e403-4fbe-bbea-caceaf4a8453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_2024.csv\", quoting = 3)\n",
    "test_comments = tf.convert_to_tensor(test_data['text'])     # type tf.Tensor\n",
    "preds = model.predict(test_comments)\n",
    "rounded_preds = np.int_( np.round(preds.flatten()) )\n",
    "\n",
    "results = test_data\n",
    "results = results.drop(['text','label'], axis=1)\n",
    "results.insert(1,'label',rounded_preds)\n",
    "\n",
    "results.to_csv('results.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dabe0d-596e-4790-b280-a82aa133460b",
   "metadata": {},
   "source": [
    "#### In case the model has already been trained and saved, we can instead evaluate it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77085d53-ec9a-45f3-bff8-9f6c6a5b2cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the model:\n",
      "Loss of the model: 0.2685219347476959\n",
      "Accuracy of the model: 0.9204545617103577\n"
     ]
    }
   ],
   "source": [
    "model = keras.saving.load_model(\"keras_model.keras\")\n",
    "\n",
    "valid_data = pd.read_csv(\"dev_2024.csv\", quoting = 3)\n",
    "valid_comments = tf.convert_to_tensor(valid_data['text'])     # type tf.Tensor\n",
    "valid_labels = tf.convert_to_tensor(valid_data['label'])      # type tf.Tensor\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(valid_comments, valid_labels, verbose = 0)\n",
    "print('Evaluation of the model:')\n",
    "print(f\"Loss of the model: {eval_loss}\")\n",
    "print(f\"Accuracy of the model: {eval_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d63c9bf-0980-4246-b114-95d5e4a1a2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/376 [==============================] - 23s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test_2024.csv\", quoting = 3)\n",
    "test_comments = tf.convert_to_tensor(test_data['text'])     # type tf.Tensor\n",
    "preds = model.predict(test_comments)\n",
    "rounded_preds = np.int_( np.round(preds.flatten()) )\n",
    "\n",
    "results = test_data\n",
    "results = results.drop(['text','label'], axis=1)\n",
    "results.insert(1,'label',rounded_preds)\n",
    "\n",
    "results.to_csv('results_cnn.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62972b-67f6-4e34-a98d-335f44ec4872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
